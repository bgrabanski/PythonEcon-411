{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "327e642f",
   "metadata": {},
   "source": [
    "# Outlining a financial valuation tool for the NDSU Bison Fund\n",
    "### with an emphasis on fundamental multiples-based valuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a643a5b",
   "metadata": {},
   "source": [
    "mandelbrot , read random walk down wall street, read martingale condition, python for finance book, EMH, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231d57e",
   "metadata": {},
   "source": [
    "Let's start with the theory behind this project, since it is the hardest part for me to wrap my own head around. This project is dedicated to detecting opportunities in public equities due to short-term emotional decisions made by stockholders. Overvaluation is the greatest enemy of good returns, and undervaluation can be a great opportunity, given the reasons for such valuation are short-term in nature and not perminantly tied into the price of the stock. It's a simple fact that all stocks have ebbs and flows, but it are those investors who take advantage of those ebbs and flows that can tip the scales in their favor when it comes to alpha generation, and true stewardship in the world of investing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79bfa2f",
   "metadata": {},
   "source": [
    "# Bibliography of theory and philosophy\n",
    "\n",
    "\n",
    "#### Sources will be put in parenthesis at the end of a statement or philisophical claim made below. Sources will be referenced back to this point with either the source # associated with the book or article, or the book/article name. This is to conserve space and make this easier for the reader to follow\n",
    "\n",
    "1. Benjamin Graham (updated Jason Zweig), initially published 1973, The Intelligent Investor: The Definitive Book on Value Investing, New York, Harper & Brothers\n",
    "\n",
    "2. Frederick K. Martin, Nick Hansen, Scott Link, and Rob Nicoski, Copyright 2012, Benhjamin Graham and the Power of Growth Stocks: Lost Growth Stock Strategies from the Father of Value Investing, McGraw-Hill Companies.\n",
    "\n",
    "3. Michail D. Vamvakaris, Athanasios A. Pantelous, Konstantin Zuev,\n",
    "Chapter 22 - Investors’ Behavior on S&P 500 Index During Periods of Market Crashes: A Visibility Graph Approach,\n",
    "Editor(s): Fotini Economou, Konstantinos Gavriilidis, Greg N. Gregoriou, Vasileios Kallinterakis,\n",
    "Handbook of Investors' Behavior During Financial Crises,\n",
    "Academic Press,\n",
    "2017,\n",
    "Pages 401-417,\n",
    "ISBN 9780128112526,\n",
    "https://doi.org/10.1016/B978-0-12-811252-6.00022-0.\n",
    "(https://www.sciencedirect.com/science/article/pii/B9780128112526000220)\n",
    "Abstract: Investors’ behavior in the market is highly related to the properties that financial time series capture. Particularly, nowadays the availability of high frequency datasets provides a reliable source for the better understanding of investors’ psychology. The main aim of this chapter is to identify changes in the persistency as well as in the local degree of irreversibility of S&P 500 price-index time series. Thus, by considering the US stock market from 1996 to 2010, we investigate how the Dot.com as well as the Subprime crashes affected investors’ behavior. Our results provide evidences that efficient market hypothesis does not hold as the high frequency S&P 500 data can be better modeled by using a fractional Brownian motion. In addition, we report that both crises only temporary effect investors’ behavior, and interestingly, before the occurrence of these two major events, the index series exhibited a kind of “nervousness” on behalf of the investors.\n",
    "Keywords: high frequency data; S&P 500; Hurst exponent; irreversibility; visibility graph method\n",
    "\n",
    "4. Ningzhong Li, Scott Richardson, İrem Tuna,\n",
    "Macro to micro: Country exposures, firm fundamentals and stock returns,\n",
    "Journal of Accounting and Economics,\n",
    "Volume 58, Issue 1,\n",
    "2014,\n",
    "Pages 1-20,\n",
    "ISSN 0165-4101,\n",
    "https://doi.org/10.1016/j.jacceco.2014.04.005.\n",
    "(https://www.sciencedirect.com/science/article/pii/S0165410114000172)\n",
    "Abstract: We outline a systematic approach to incorporate macroeconomic information into firm level forecasting from the perspective of an equity investor. Using a global sample of 198,315 firm-years over the 1998–2010 time period, we find that combining firm level exposures to countries (via geographic segment data) with forecasts of country level performance, is able to generate superior forecasts for firm fundamentals. This result is particularly evident for purely domestic firms. We further find that this forecasting benefit is associated with future excess stock returns. These relations are stronger after periods of higher dispersion in expected country level performance.\n",
    "Keywords: Macroeconomic exposures; Earnings; Stock returns; Geographic segments\n",
    "\n",
    "### Added for opposition purposes\n",
    "\n",
    "5. Kenneth R. French, G.William Schwert, Robert F. Stambaugh,\n",
    "Expected stock returns and volatility,\n",
    "Journal of Financial Economics,\n",
    "Volume 19, Issue 1,\n",
    "1987,\n",
    "Pages 3-29,\n",
    "ISSN 0304-405X,\n",
    "https://doi.org/10.1016/0304-405X(87)90026-2.\n",
    "(https://www.sciencedirect.com/science/article/pii/0304405X87900262)\n",
    "Abstract: This paper examines the relation between stock returns and stock market volatility. We find evidence that the expected market risk premium (the expected return on a stock portfolio minus the Treasury bill yield) is positively related to the predictable volatility of stock returns. There is also evidence that unexpected stock market returns are negatively related to the unexpected change in the volatility of stock returns. This negative relation provides indirect evidence of a positive relation between expected risk premiums and volatility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b02c4d",
   "metadata": {},
   "source": [
    "Now you may be asking, \"Bryce, are you assuming that you can time the market despite hundreds of investors saying otherwise? Are you really going to spit in the face of the Efficient Market Hypothesis as a college sophomore?\" Well the answer is yes. Although, I do have some caviats to that claim. For this example I will use Stamps.com. This company has since gone private, so it works out as a good example. Stamps.com was a phenomenal company, which became a really revolutionary service provider of internet-based postage. This allowed customers to buy and print postage online, which saved time and money for a lot of consumers. This allowed businesses to print at their computer, enabled small businesses, home offices, and online retailers to shift much quicker and more efficiently. In addition, it also offered a cost-benefit to their customers. The best part was that their competition was basically none. Well stamps.com has had several price corrections which do not display market efficiency. The first is obvious in 2019 when stamps.com ended their exclusivity agreement with USPS, which despite analysts expectations allowed their business to grow at a rate unseen before. This was a false reading of a market signal, and because of this market inefficiency, allowed investors who understood their business more closely to take advantage.\n",
    "\n",
    "Our theory is that Stamps.com is not the only stock that has been priced poorly due to one or more of the following:\n",
    "1. Fundamental misunderstanding of what the company does to generate value, which we define as anything that influences the intrinsic value of the company. This includes day-to-day business decisions, processes, and defensibility (Benjamin Graham and the Power of Growth Stocks)\n",
    "2. Large changes in investor sentiment or emotional impact. This means that a stock may be overbought or oversold based on the human biases that affect all investors during an investment. These events are often not grounded in fundamental data that impacts the value of a company (The Intelligent Investor)\n",
    "3. Inefficiency of markets due to any external pressures. This includes customer pressure to invest or pull out of a certain investment, political pressures, taxation pressures, or other behavioral pressures. These pressures can create conditions where it is more beneficial for institutions or individuals to make short-term investment decisions that are poor long-term investment decisions. One example of which would be a withdraw of funds after a retirement account reaches an all-time-low due to emotional biases.\n",
    "\n",
    "For the purposes of this project, we are going to assume that Behavioral finance theory is observable and commonplace. This is especially true of less-liquid or less-viewed areas in finance such as small-mid cap equities. \n",
    "\n",
    "\n",
    "### Below is an excerpt from the motley fool the day after Stamps.com dropped 55.7% overnight.\n",
    "\n",
    "The good news: Q4 earnings were just fine. Customer churn rates declined to a minuscule 2.9%, while average revenues per customer and total revenue for the quarter both surged 29% year over year, and profits climbed a modest 6%.\n",
    "\n",
    "The bad news: None of that matters, because Stamps.com is terminating its exclusive deal to sell postage for the partner that accounts for the vast majority of its business, the U.S. Postal Service. Stamps.com stock is down 55.7% as of 11:35 a.m. EST in response.\n",
    "\n",
    "source: https://www.fool.com/investing/2019/02/22/why-stampscom-stock-dropped-55-overnight.aspx\n",
    "\n",
    "### Takeaway\n",
    "\n",
    "I think this article shows my point fairly clearly. The company made very good fundamental progress, their customer churn was astoundingly low for a tech-y company, and my personal favorite: Their per-customer revenues went up 29% YOY. It is hard to say that the company didn't make fundamental progress, and though many people were worried about the future of the company without USPS, within a year it began it's pricing climb past it's previous hights. Stamps.com was a great company, and it's problems were short acting.\n",
    "\n",
    "I am willing to concede that this move established a certain amount of uncertainty when it came to the company. Estimated profits for the next year declined nearly 60% from the management team, which is exactly what made this such a brilliant buying opportunity. My opinion is that since so many investors are concerned with the short-term happenings of the market there are far more opportunities in the long-term. This management team could've coasted by on bad deals with the USPS, and nobody would fault them. However, they saw the long-term growth prospects of their business from going independent and being able to work with more than just USPS. This was a phenomenal move for the company, but the industry saw it as a short-term hinderance to the value of the stock.\n",
    "\n",
    "Thus, those investors were extraordinarily blessed in the long-term for those that bought in at this low. This is the exact kind of opportunity that we want to look for. A company that has high potential long-term, but is oversold because of a short-term, non-perminant reason that has nothing to do with the foundation of the company.\n",
    "\n",
    "### Accepted Theories and Philosophies\n",
    "\n",
    "I am very suspicious of anybody who claims that the market is perfectly efficient, and that there is no way to outperform the market over a long period of time. There are pricing changes that make no sense all the time. Sometimes a company is trading at .3x book value with positive growth and incomes. The people who bought ENRON during it's liquidation made 30%. The market often is not perfectly efficient. With this in mind, we are assuming that:\n",
    "\n",
    "\n",
    "-behavioral finance theory is the basis of our assumption that prices are not perfectly reflected at any given time due to behavioral biases and other interference (Intelligent Investor & Ben Graham and the Power of Growth Stocks)\n",
    "\n",
    "-We assume prospect theory is strongly in play, due to the loss-aversion of typical people. Since risk is stronly quantified in industry as beta, and we do not agree with this assessment of risk (for long-term investing), we are not as subject to this behavioral bias as other groups. (Source #3)\n",
    "\n",
    "-We accept that the greater-fools theory is one of many examples of false-pricing due to no reguard to fundamentals. This is especially strong on the pull of the stock price during times of significant optimism, sometimes before a correction. (Benjamin Graham and the Power of Growth Stocks, Source #3)\n",
    "\n",
    "-We define risk as the posibility of a total loss of capital. This could be due to fundamental problems in a business, fraudulent accounts, valuation risk, market risk, liquidity risk, and many others that fall into this category. However, beta will be excluded from the evaluation of risk in a company, since we assume stock-price is not a perfect indicator of a company's value in the short-term, and we are more afraid of a perminant loss of capital than a short-term undervaluation of our company by the market. \n",
    "\n",
    "-the short-term price of a stock does not perfectly reflect it's underlying value (Benjamin Graham and the Power of Growth Stocks, The Intelligent Investor, Source #3)\n",
    "\n",
    "-the long-term price of a stock should true up with it's underlying value (Benjamin Graham and the Power of Growth Stocks, The Intelligent Investor)\n",
    "\n",
    "\n",
    "#### Cautionary Rules\n",
    "\n",
    "These economic models are by no means to be used for stock trading. I guarentee there are bugs, biases, and shortcomings of what I was able to do in a few weeks of coding. \n",
    "\n",
    "**THIS IS ONLY A TOOL TO BE USED ALONG WITH PERSONAL DUE DILIGENCE**\n",
    "\n",
    "any stock recommended by this model is not to be bought with no second thought. The single greatest risk management tool you have is to understand the company and their intrinsic value. No recommendations made by this model are to be followed. It is not my fault if you lose a bunch of money because you looked at a random guy's github assignment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8c0463",
   "metadata": {},
   "source": [
    "My vision for my final project for Econ-411 is an interactive stock analysis dashboard that could be institutionalized and used for Bison Fund analysis. My thoughts of useful content is split up into 4 categories\n",
    "1.\tFundamental Analysis\n",
    "\n",
    "Fundamental Analysis is the basis of Bison Fund’s security Analysis. Having the opportunity to pull data from SEC 10k and 10q filings would both speed up our process as well as give better fundamental data. This data would then be filtered through to allow us to get key performance indicators including company margins over time, ROA, ROIC, and ROE metrics, which we feel is indicative of how well a company uses shareholder capital, and other important information about the company. Employee count, square footage being used, and segmented data would all be helpful, but I expect it to be difficult to create a methodology to scrub that across more than an individual company, and it may need to be on a case by case basis. Especially with changes in reporting and methodology this might not be feasible. If possible, I would also like to be able to pull company descriptions, supply chain data, and other useful statistics that have to do with fundamental and operational data for each company. Unfortunately I am unsure if this exists, so this is on the backburner for now.\n",
    "This fundamental data will probably come from a dataset provided by SEC, which gives a full dataset of all quarterly filings that are sent in to them. This dataset is quite large and will need to be converted into a parquet to be usable in python, as well as make it user friendly for specific security lookup. I will try to build a simple valuation model into this analysis using EPS data and assigning a multiple based on expected EPS growth or ROIC, whichever I decide is more indicative of a premium that should be paid for a company.\n",
    "\n",
    "2.\tPortfolio Backdrop\n",
    "\n",
    "It is important to have a perception of how well that the Bison Fund is actually selecting securities. Not only are we intending to be introspective into how well we actually picked individual stocks, but also how well we projected their growth going forward. That is why we would like to have a comparison of the performance of our portfolio vs. our investible world. For the comparison, I would like to use the Russel 2000 as an index since we focus on the small – mid cap space, and my intention is to run a simple simulation of different portfolios to see their performance as risk adjusted. As of now I’m not sure how I would like to quantify risk in each portfolio since I don’t see volatility as risk if the company is held at an average of 7 years. At that point it’s only valuation risk that we’re afraid of.\n",
    "I expect this testing to use a dataset from Bloomberg or yfinance to pull prices, log the data of the individual companies in the Russel 2000, and then compare randomly weighted portfolio performance of around 25 companies with the Russel 2000 index as a baseline, and also the S&P as a baseline. Maybe this will generate more questions or insights into what type of companies we should be looking to pitch back to the fund.\n",
    "\n",
    "3.\tMacroeconomic awareness \n",
    "\n",
    "One of the enemies that we all face is Mr. Market. Valuation risk is a real risk, and oftentimes the market sentiment overestimates the optimism and the pessimism of each individual security. You need only look at Cisco at the turn of the century. A security that could never produce enough value to be worth $80 per share was squeezed way higher because of market sentiment. It hasn’t recovered since because of fundamental reasons. We want to be able to determine the healthiness of the market as a % of stocks in the index that have gone up vs down. I am hoping to set an arbitrary amount like 20% in either direction, and then hold a third option as relatively no change. As of now I want to annualize the data and put it over a 3 year time frame, but this is subject to change as I see fit to adjust. This will give us insight into how investible our world of stocks is, since if 90% of our investible world is down 20% or more YOY, the other 10% are neutral, and the company fundamentals have not changed, this gives a strong signal to purchase overselling of phenomenal companies.\n",
    "I expect this will use a similar dataset to bullet #2, by just pulling pricing data of all the stocks in the Russel 2000 and then having a rolling number that states the count or % as Optimism or Pessimism.\n",
    "\n",
    "4.\tLive pricing data for current portfolio\n",
    "\n",
    "I already have the bone structure of this process for obvious reasons. We want to be able to check pricing data of our current holdings and compare them to intrinsic values of our companies. This will give us a buy or sell signal on individual securities. \n",
    "This will use the yfinance database through python, and can return dataframes for individual securities and grouped securities.\n",
    "\n",
    "Below are exercises of me pulling historical pricing data from different stocks that Bison Fund has considered investing in, data to pull pricing data about individual stocks. Previously I had a working function to pull multiple at once, but since reverting to an older version of yfinance this has ceased to be useful. I have charted log data of stock prices, a dataset of expected earnings, which we will use to estimate intrinsic value if we don't have one, and assign multiples to potential investments. I also have imported CPI data from fred, which I might use for macroeconomic backdrop. As of yet I haven't decided. There also is a potential to use bloomberg API if I can learn it quick enough for the final project deadline.\n",
    "\n",
    "This is all to be used for Bison Fund and the production of value for our managed funds, the aid of students in understanding the process of stock analysis better, and for the sake of improving the due diligence process of the Fund. \n",
    "\n",
    "Hopefully through this dashboard we can make more timely decisions on purchasing and selling securities, have a better introspective view on how well the fund is doing at security selection, and make better buying decisions based on overexaggerated sentiment of the market in the short term to create value over the long term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7b7971",
   "metadata": {},
   "source": [
    "## Development Notes & Future Goals\n",
    "\n",
    "**Note that development is not finished at this point in time**\n",
    "\n",
    "future goals for this project include:\n",
    "\n",
    "1. adding other forms of valuation including DCF analysis, other multiples analysis, and comparison among competitors automatically. This will allow for more confidence in the intrinsic values, and hopefully can be tweaked to a default value for all of the assumptions which would be needed for this analysis. This would allow us to have more accurate readings of value for each stock put through this analysis, and would free up analyst time for more qualitative data analysis since that is more difficult to code due to unavailable data, difficulty managing said data, and difficulty separating useful from unuseful data. Perhaps there is some chat GPT thing to be done here, but probably not in the near term...\n",
    "\n",
    "2. exporting different pdfs of graphs, research, etc. which would allow for easier to understand presentations, notes, designated investment memos, etc. This would allow for automation of some of the most menial tasks that we face in the bison fund which are doing calculations and graphing of this data over time. Automating this would be as easy as setting a methodology for graphing the above, running it on every stock in our investible world, and then exporting the data to pdf along with a screenshot of our valuation model. This would allow the analyst to write in qualitative analysis exclusively, and would do all computation and graphical placements automatically. Lots of potential for balance sheet, income statement, cash flow, and financial metrics data over time, as well as automating sensitivity charts, pie charts, competitor multiples tables and much more. This one is not only doable, but would bring the most value in the shortest amount of time investment.\n",
    "\n",
    "3. hopes to tie plotly and dash into this project were thrown around in early development, however time constraints seem to have limited the ability to fulfill these wishes. Optimally, there would be a refresh button to pull new pricing data from yfinance or fundamental data from alpha vantage in the background while the old data was still visible. \n",
    "\n",
    "4. UNSURE IF POSSIBLE: Potential incorporation with our portfolio data from our custodian accounts of our investments. Showing cost-basis, transaction dates, and this all would lead to number 5 if possible.\n",
    "\n",
    "5. database and evaluation of how good our decisions were. Whether they were buy, sell, or non-decisions, it would be helpful to be able to compile data from analysts that had opinions or discussion about a decision, and then be able to evaluate those over the course of years so that newer students could be able to look back and learn from analysts' mistakes from years gone past. When coupled with our financial data and fundamentals, we could potentially find different weakpoints in the way we analyze companies and value them. Whether that would pertain to biases that we hold closely or fundamental mistakes in our modeling techniques, this would be integral to improving the process and abilities of our analysts over time. Especially useful since there is so much turnover in our area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5b813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2932676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pandas_datareader as web\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d96f5f3",
   "metadata": {},
   "source": [
    "# Fundamental Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e15627c",
   "metadata": {},
   "source": [
    "### Retrieving Fundamental Data through Alpha Vantage's API\n",
    "\n",
    "First we need a way to retrieve the data that we find most substantial for determining a company's value. \n",
    "\n",
    "Since we are assuming that the greater fool's theory exists, but is a extraordinarily risky style of stock trading, we are treating the prospect of trading with no fundamental backdrop to be the highest risk, or highest probability of a perminant loss of capital.\n",
    "\n",
    "Thus, this fundamental progression data of a company, as taken through their 10ks and 10qs will be foundational for us to quantify a company's intrinsic value. This will be calculated in a variety of ways with the intention of having a process that is easy to calculate and interpret. Thus, most valuation data will be reported on a per share basis. This allows numbers to be more easily interpretable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6572af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"TULH49F5XQMU8MQ5\"\n",
    "base_url = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "# first we must create a function that retrieves the important data that\n",
    "# we need. get_fin_stmt will be the basis of our process, which uses the \n",
    "# keys in the API to retrieve json data back that has been arranged by analysts.\n",
    "# the data has been accurate when checked against 10k data for several stocks,\n",
    "# but the values may be off. This is something to keep in mind and check \n",
    "# occasionally.\n",
    "\n",
    "def get_fin_stmt(symbol, function):\n",
    "    url = base_url + \"?function=\" + function + \"&symbol=\" + symbol + \"&apikey=\" + api_key\n",
    "    response = requests.get(url)\n",
    "    time.sleep(16)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error, cannot retrieve\" + function + \"for \" + symbol)\n",
    "        return None\n",
    "    data = response.json()\n",
    "    if \"annualReports\" not in data:\n",
    "        print(\"Error: No \" + function + \" data found for \" + symbol)\n",
    "        return None\n",
    "#     print(data)\n",
    "    statement_data = data[\"annualReports\"]\n",
    "    return_dct = {}\n",
    "    \n",
    "    return statement_data\n",
    "statement_dct = {}\n",
    "\n",
    "\n",
    "# Now we must create a dataframe to store all of our stock data, as well as\n",
    "# express the stocks that we wish to research\n",
    "statements = {}\n",
    "stocks = [\"ENPH\", \"IBM\", \"MBUU\", \"MANH\", \"TREX\", \"LSCC\"]\n",
    "\n",
    "# state the different statement types we want to cycle through\n",
    "statement_types =  [\"INCOME_STATEMENT\", \"BALANCE_SHEET\", \"CASH_FLOW\"] #,\"EARNINGS\"\n",
    "\n",
    "# Note that because of API limitations, we are only allowed to make 5 API\n",
    "# requests per minute with our free version. \n",
    "\n",
    "# Future versions of this code\n",
    "# should make a parquay of previous data, so that we can use that for past\n",
    "# and future valuations without needing to redownload the data. This hopefully\n",
    "# will also be usable for a Discounted Cash Flow model to be created in this\n",
    "# format, which would also export to pdf for easy access to graphs, financial\n",
    "# analysis which is automated for analysts, and an easy-to-read methodology\n",
    "# that fixes many of the issues with excel.\n",
    "\n",
    "for stock in stocks:\n",
    "    statements[stock] = {}\n",
    "    for statement_type in statement_types:\n",
    "        statement_key = statement_type.split(\"_\")[0]\n",
    "        statements[stock][statement_key] = {}\n",
    "        try:\n",
    "            fin_statement = get_fin_stmt(stock, statement_type)\n",
    "            for annual_data in fin_statement:\n",
    "                date = annual_data[\"fiscalDateEnding\"]\n",
    "\n",
    "                statements[stock][statement_key][date] = annual_data\n",
    "        except:\n",
    "            print(\"Error retrieving \" + statement_type + \" for \" + stock)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "619c0a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fiscalDateEnding</th>\n",
       "      <th>reportedCurrency</th>\n",
       "      <th>grossProfit</th>\n",
       "      <th>totalRevenue</th>\n",
       "      <th>costOfRevenue</th>\n",
       "      <th>costofGoodsAndServicesSold</th>\n",
       "      <th>operatingIncome</th>\n",
       "      <th>sellingGeneralAndAdministrative</th>\n",
       "      <th>researchAndDevelopment</th>\n",
       "      <th>operatingExpenses</th>\n",
       "      <th>...</th>\n",
       "      <th>depreciation</th>\n",
       "      <th>depreciationAndAmortization</th>\n",
       "      <th>incomeBeforeTax</th>\n",
       "      <th>incomeTaxExpense</th>\n",
       "      <th>interestAndDebtExpense</th>\n",
       "      <th>netIncomeFromContinuingOperations</th>\n",
       "      <th>comprehensiveIncomeNetOfTax</th>\n",
       "      <th>ebit</th>\n",
       "      <th>ebitda</th>\n",
       "      <th>netIncome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-12-31</th>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>USD</td>\n",
       "      <td>974595000</td>\n",
       "      <td>2330853000</td>\n",
       "      <td>1356258000</td>\n",
       "      <td>1356258000</td>\n",
       "      <td>448261000</td>\n",
       "      <td>355104000</td>\n",
       "      <td>168846000</td>\n",
       "      <td>526334000</td>\n",
       "      <td>...</td>\n",
       "      <td>27700000</td>\n",
       "      <td>24696000</td>\n",
       "      <td>452048000</td>\n",
       "      <td>54686000</td>\n",
       "      <td>9438000</td>\n",
       "      <td>397362000</td>\n",
       "      <td>388500000</td>\n",
       "      <td>461486000</td>\n",
       "      <td>486182000</td>\n",
       "      <td>397362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-31</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>USD</td>\n",
       "      <td>554422000</td>\n",
       "      <td>1382049000</td>\n",
       "      <td>827627000</td>\n",
       "      <td>827627000</td>\n",
       "      <td>215832000</td>\n",
       "      <td>233064000</td>\n",
       "      <td>105526000</td>\n",
       "      <td>338590000</td>\n",
       "      <td>...</td>\n",
       "      <td>16700000</td>\n",
       "      <td>9500000</td>\n",
       "      <td>120928000</td>\n",
       "      <td>-24521000</td>\n",
       "      <td>101649000</td>\n",
       "      <td>145449000</td>\n",
       "      <td>142995000</td>\n",
       "      <td>166080000</td>\n",
       "      <td>175580000</td>\n",
       "      <td>145449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>USD</td>\n",
       "      <td>345981000</td>\n",
       "      <td>774425000</td>\n",
       "      <td>428444000</td>\n",
       "      <td>428444000</td>\n",
       "      <td>186439000</td>\n",
       "      <td>103621000</td>\n",
       "      <td>55921000</td>\n",
       "      <td>159542000</td>\n",
       "      <td>...</td>\n",
       "      <td>9700000</td>\n",
       "      <td>5092000</td>\n",
       "      <td>119410000</td>\n",
       "      <td>-14585000</td>\n",
       "      <td>21001000</td>\n",
       "      <td>133995000</td>\n",
       "      <td>135352000</td>\n",
       "      <td>140411000</td>\n",
       "      <td>145503000</td>\n",
       "      <td>133995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>USD</td>\n",
       "      <td>221245000</td>\n",
       "      <td>624333000</td>\n",
       "      <td>403088000</td>\n",
       "      <td>403088000</td>\n",
       "      <td>102729000</td>\n",
       "      <td>75536000</td>\n",
       "      <td>40381000</td>\n",
       "      <td>118516000</td>\n",
       "      <td>...</td>\n",
       "      <td>7300000</td>\n",
       "      <td>4727000</td>\n",
       "      <td>90114000</td>\n",
       "      <td>-71034000</td>\n",
       "      <td>17843000</td>\n",
       "      <td>161148000</td>\n",
       "      <td>159483000</td>\n",
       "      <td>99805000</td>\n",
       "      <td>104532000</td>\n",
       "      <td>161148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>USD</td>\n",
       "      <td>94445000</td>\n",
       "      <td>316159000</td>\n",
       "      <td>221714000</td>\n",
       "      <td>221714000</td>\n",
       "      <td>1596000</td>\n",
       "      <td>56133000</td>\n",
       "      <td>32587000</td>\n",
       "      <td>92849000</td>\n",
       "      <td>...</td>\n",
       "      <td>8300000</td>\n",
       "      <td>1400000</td>\n",
       "      <td>-10229000</td>\n",
       "      <td>1398000</td>\n",
       "      <td>9635000</td>\n",
       "      <td>-11627000</td>\n",
       "      <td>-10229000</td>\n",
       "      <td>464000</td>\n",
       "      <td>1864000</td>\n",
       "      <td>-11627000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           fiscalDateEnding reportedCurrency grossProfit totalRevenue  \\\n",
       "2022-12-31       2022-12-31              USD   974595000   2330853000   \n",
       "2021-12-31       2021-12-31              USD   554422000   1382049000   \n",
       "2020-12-31       2020-12-31              USD   345981000    774425000   \n",
       "2019-12-31       2019-12-31              USD   221245000    624333000   \n",
       "2018-12-31       2018-12-31              USD    94445000    316159000   \n",
       "\n",
       "           costOfRevenue costofGoodsAndServicesSold operatingIncome  \\\n",
       "2022-12-31    1356258000                 1356258000       448261000   \n",
       "2021-12-31     827627000                  827627000       215832000   \n",
       "2020-12-31     428444000                  428444000       186439000   \n",
       "2019-12-31     403088000                  403088000       102729000   \n",
       "2018-12-31     221714000                  221714000         1596000   \n",
       "\n",
       "           sellingGeneralAndAdministrative researchAndDevelopment  \\\n",
       "2022-12-31                       355104000              168846000   \n",
       "2021-12-31                       233064000              105526000   \n",
       "2020-12-31                       103621000               55921000   \n",
       "2019-12-31                        75536000               40381000   \n",
       "2018-12-31                        56133000               32587000   \n",
       "\n",
       "           operatingExpenses  ... depreciation depreciationAndAmortization  \\\n",
       "2022-12-31         526334000  ...     27700000                    24696000   \n",
       "2021-12-31         338590000  ...     16700000                     9500000   \n",
       "2020-12-31         159542000  ...      9700000                     5092000   \n",
       "2019-12-31         118516000  ...      7300000                     4727000   \n",
       "2018-12-31          92849000  ...      8300000                     1400000   \n",
       "\n",
       "           incomeBeforeTax incomeTaxExpense interestAndDebtExpense  \\\n",
       "2022-12-31       452048000         54686000                9438000   \n",
       "2021-12-31       120928000        -24521000              101649000   \n",
       "2020-12-31       119410000        -14585000               21001000   \n",
       "2019-12-31        90114000        -71034000               17843000   \n",
       "2018-12-31       -10229000          1398000                9635000   \n",
       "\n",
       "           netIncomeFromContinuingOperations comprehensiveIncomeNetOfTax  \\\n",
       "2022-12-31                         397362000                   388500000   \n",
       "2021-12-31                         145449000                   142995000   \n",
       "2020-12-31                         133995000                   135352000   \n",
       "2019-12-31                         161148000                   159483000   \n",
       "2018-12-31                         -11627000                   -10229000   \n",
       "\n",
       "                 ebit     ebitda  netIncome  \n",
       "2022-12-31  461486000  486182000  397362000  \n",
       "2021-12-31  166080000  175580000  145449000  \n",
       "2020-12-31  140411000  145503000  133995000  \n",
       "2019-12-31   99805000  104532000  161148000  \n",
       "2018-12-31     464000    1864000  -11627000  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_df = pd.DataFrame(statements[\"ENPH\"][\"INCOME\"]).T\n",
    "plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd3d225e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fiscalDateEnding', 'reportedCurrency', 'grossProfit', 'totalRevenue',\n",
       "       'costOfRevenue', 'costofGoodsAndServicesSold', 'operatingIncome',\n",
       "       'sellingGeneralAndAdministrative', 'researchAndDevelopment',\n",
       "       'operatingExpenses', 'investmentIncomeNet', 'netInterestIncome',\n",
       "       'interestIncome', 'interestExpense', 'nonInterestIncome',\n",
       "       'otherNonOperatingIncome', 'depreciation',\n",
       "       'depreciationAndAmortization', 'incomeBeforeTax', 'incomeTaxExpense',\n",
       "       'interestAndDebtExpense', 'netIncomeFromContinuingOperations',\n",
       "       'comprehensiveIncomeNetOfTax', 'ebit', 'ebitda', 'netIncome'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6faaf2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now need to create a Calculate ROIC equation. Note the below\n",
    "# is removing Dividends from the valuation because we will factor those \n",
    "# in later by adding dividend yield to our stock valuation. These figures\n",
    "# will be discounted for a margin of safety, but will also allow us to see\n",
    "# potential annual returns, when assuming a 7-year period before we true up\n",
    "# to the basis value of a stock.\n",
    "\n",
    "def calc_ROIC(EBIT, Taxes, Dividends, Debt, Equity, Cash_Equiv):\n",
    "    try:\n",
    "        NOPAT = (int(EBIT) - int(Taxes)) - int(Dividends)\n",
    "        Invested_Capital = int(Debt) + int(Equity) - int(Cash_Equiv)\n",
    "        ROIC = round(NOPAT / Invested_Capital, 2)\n",
    "    except:\n",
    "        NOPAT = (int(EBIT) - int(Taxes))\n",
    "        Invested_Capital = int(Debt) + int(Equity) - int(Cash_Equiv)\n",
    "        ROIC = round(NOPAT / Invested_Capital, 2)\n",
    "    return ROIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbdfd35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create calc_EPS equation that can calculate earnings per share. Once again\n",
    "# avoiding dividends in the equation to prevent double counting.\n",
    "\n",
    "def calc_EPS(Net_Income, Dividends, Shares):\n",
    "    try:\n",
    "        EPS = round((int(Net_Income) - int(Dividends)) / int(Shares),2)\n",
    "    except:\n",
    "        EPS = round((int(Net_Income)) / int(Shares),2)\n",
    "    return EPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a92bac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ROI(Net_Income, Debt, Equity, Cash_Equiv):\n",
    "    Invested_Capital = int(Debt) + int(Equity) - int(Cash_Equiv)\n",
    "    ROI = Net_Income / Invested_Capital\n",
    "    return ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cd04458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_Current_Ratio(Current_Assets, Current_Liabilities):\n",
    "    Current_Ratio = round(int(Current_Assets) / int(Current_Liabilities),2)\n",
    "    return Current_Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6c22a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core of valuation, subject to change suddenly over new valuation \n",
    "# methodologies. Hoping to have projected EPS based on growth rates and \n",
    "# diminishing linear projection for EPS and also ROIC numbers. Assigned\n",
    "# multiple will be based on growth rates & ROIC in some way\n",
    "\n",
    "# note 60 currently a placeholder in the valuation process. This will give\n",
    "# wildly varying valuations based on EPS due to several reasons. Future \n",
    "# methodology will allow us to throttle this valuation more easily.\n",
    "\n",
    "def EPS_Valuation(EPS, ROIC):\n",
    "    Multiple = (8.5 + 60*(ROIC))\n",
    "    Price = round(Multiple * EPS,2)\n",
    "    return Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57a091b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406800000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calcula\n",
    "#calc_ROIC(statements[\"ENPH\"][\"INCOME\"][\"2022-12-31\"][\"ebit\"], )\n",
    "int(statements[\"ENPH\"][\"INCOME\"][\"2022-12-31\"][\"ebit\"]) - int(statements[\n",
    "    \"ENPH\"][\"INCOME\"][\"2022-12-31\"][\"incomeTaxExpense\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc7237d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fiscalDateEnding', 'reportedCurrency', 'grossProfit', 'totalRevenue', 'costOfRevenue', 'costofGoodsAndServicesSold', 'operatingIncome', 'sellingGeneralAndAdministrative', 'researchAndDevelopment', 'operatingExpenses', 'investmentIncomeNet', 'netInterestIncome', 'interestIncome', 'interestExpense', 'nonInterestIncome', 'otherNonOperatingIncome', 'depreciation', 'depreciationAndAmortization', 'incomeBeforeTax', 'incomeTaxExpense', 'interestAndDebtExpense', 'netIncomeFromContinuingOperations', 'comprehensiveIncomeNetOfTax', 'ebit', 'ebitda', 'netIncome'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keys as are stored in each of the different statements next 3 cells\n",
    "statements[\"ENPH\"][\"INCOME\"][\"2022-12-31\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2e824a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fiscalDateEnding', 'reportedCurrency', 'totalAssets', 'totalCurrentAssets', 'cashAndCashEquivalentsAtCarryingValue', 'cashAndShortTermInvestments', 'inventory', 'currentNetReceivables', 'totalNonCurrentAssets', 'propertyPlantEquipment', 'accumulatedDepreciationAmortizationPPE', 'intangibleAssets', 'intangibleAssetsExcludingGoodwill', 'goodwill', 'investments', 'longTermInvestments', 'shortTermInvestments', 'otherCurrentAssets', 'otherNonCurrentAssets', 'totalLiabilities', 'totalCurrentLiabilities', 'currentAccountsPayable', 'deferredRevenue', 'currentDebt', 'shortTermDebt', 'totalNonCurrentLiabilities', 'capitalLeaseObligations', 'longTermDebt', 'currentLongTermDebt', 'longTermDebtNoncurrent', 'shortLongTermDebtTotal', 'otherCurrentLiabilities', 'otherNonCurrentLiabilities', 'totalShareholderEquity', 'treasuryStock', 'retainedEarnings', 'commonStock', 'commonStockSharesOutstanding'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statements[\"ENPH\"][\"BALANCE\"][\"2022-12-31\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56fc557c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fiscalDateEnding', 'reportedCurrency', 'operatingCashflow', 'paymentsForOperatingActivities', 'proceedsFromOperatingActivities', 'changeInOperatingLiabilities', 'changeInOperatingAssets', 'depreciationDepletionAndAmortization', 'capitalExpenditures', 'changeInReceivables', 'changeInInventory', 'profitLoss', 'cashflowFromInvestment', 'cashflowFromFinancing', 'proceedsFromRepaymentsOfShortTermDebt', 'paymentsForRepurchaseOfCommonStock', 'paymentsForRepurchaseOfEquity', 'paymentsForRepurchaseOfPreferredStock', 'dividendPayout', 'dividendPayoutCommonStock', 'dividendPayoutPreferredStock', 'proceedsFromIssuanceOfCommonStock', 'proceedsFromIssuanceOfLongTermDebtAndCapitalSecuritiesNet', 'proceedsFromIssuanceOfPreferredStock', 'proceedsFromRepurchaseOfEquity', 'proceedsFromSaleOfTreasuryStock', 'changeInCashAndCashEquivalents', 'changeInExchangeRate', 'netIncome'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statements[\"ENPH\"][\"CASH\"][\"2022-12-31\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af0b5789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we calculate a core component of our valuation strategy, ROIC\n",
    "# We believe that ROIC is a good indicator of return to shareholder capital,\n",
    "# and in our thesis shows what the return on the money generated by the \n",
    "# business could potentially be. This will be used as a guage for our assigned\n",
    "# multiple for businesses. I am also adding a cash&Equivalents portion\n",
    "# to artificially inflate businesses with higher cash stores. This\n",
    "# is meant to give a higher multiple to cash-neutral or cash-holding businesses\n",
    "# due to decreased liquidity risk. Might tweak this later.\n",
    "\n",
    "# right now we are removing dividends from the EPS calculation since I will be adding dividend yields into the \n",
    "# valuation later on if I have time. Note: underestimating multiples for some companies, but strongly on dividend companies\n",
    "\n",
    "\n",
    "for stock in statements:\n",
    "    statements[stock][\"VALUATION\"] = {}\n",
    "    for date in statements[stock][\"BALANCE\"]:\n",
    "            statements[stock][\"VALUATION\"][date] = {}\n",
    "            new = statements[stock][\"VALUATION\"][date]\n",
    "            old = statements[stock]\n",
    "            # create ROIC values in Valuation\n",
    "            new[\"ROIC\"] = calc_ROIC(EBIT = old[\"INCOME\"][date][\"ebit\"], \n",
    "                                    Taxes = old[\"INCOME\"][date][\"incomeTaxExpense\"],\n",
    "                                    Dividends = old[\"CASH\"][date][\"dividendPayout\"],\n",
    "                                    Debt = old[\"BALANCE\"][date][\"totalLiabilities\"],\n",
    "                                   Equity = old[\"BALANCE\"][date][\"totalShareholderEquity\"],\n",
    "                                   Cash_Equiv = old[\"BALANCE\"][date][\"cashAndShortTermInvestments\"])\n",
    "            \n",
    "            # create EPS values in Valuation\n",
    "            new[\"EPS\"] = calc_EPS(Net_Income = old[\"INCOME\"][date][\"netIncome\"], \n",
    "                                   Dividends = old[\"CASH\"][date][\"dividendPayout\"],\n",
    "                                  Shares = old[\"BALANCE\"][date][\"commonStockSharesOutstanding\"])\n",
    "            \n",
    "            # create current ratio values in valuation\n",
    "            new[\"Current_Ratio\"] = calc_Current_Ratio(Current_Assets = old[\"BALANCE\"][date][\"totalCurrentAssets\"],\n",
    "                                                      Current_Liabilities = old[\"BALANCE\"][date][\"totalCurrentLiabilities\"])\n",
    "            statements[stock][\"VALUATION\"][date][\"EPS_Valuation\"] = EPS_Valuation(EPS = new[\"EPS\"],\n",
    "                                                                                 ROIC = new[\"ROIC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5169d64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting parquet\n",
      "  Downloading parquet-1.3.1-py3-none-any.whl (24 kB)\n",
      "Collecting thriftpy2\n",
      "  Downloading thriftpy2-0.4.16.tar.gz (643 kB)\n",
      "     -------------------------------------- 643.4/643.4 kB 5.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting ply<4.0,>=3.4\n",
      "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "     ---------------------------------------- 49.6/49.6 kB 2.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six~=1.15 in c:\\users\\bryce grabanski\\anaconda3\\lib\\site-packages (from thriftpy2->parquet) (1.16.0)\n",
      "Building wheels for collected packages: thriftpy2\n",
      "  Building wheel for thriftpy2 (setup.py): started\n",
      "  Building wheel for thriftpy2 (setup.py): finished with status 'done'\n",
      "  Created wheel for thriftpy2: filename=thriftpy2-0.4.16-py2.py3-none-any.whl size=337225 sha256=ac59c22ed057cf9e1a0aab1fc075a3b4f509993468dfb220a73ead9a16cb7400\n",
      "  Stored in directory: c:\\users\\bryce grabanski\\appdata\\local\\pip\\cache\\wheels\\88\\a4\\d5\\907737b4c175aec82087b815fa93a8afea5c6c5a3e7bb748b9\n",
      "Successfully built thriftpy2\n",
      "Installing collected packages: ply, thriftpy2, parquet\n",
      "Successfully installed parquet-1.3.1 ply-3.11 thriftpy2-0.4.16\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11712\\1471399085.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m    \u001b[1;31m#     except:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m#        pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatements\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstock\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstatement_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstock\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstatement_type\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".parquet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mto_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   2833\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_parquet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2835\u001b[1;33m         return to_parquet(\n\u001b[0m\u001b[0;32m   2836\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2837\u001b[0m             \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartition_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[0mpartition_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpartition_cols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m     \u001b[0mimpl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFilePath\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mWriteBuffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mget_engine\u001b[1;34m(engine)\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0merror_msgs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"\\n - \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         raise ImportError(\n\u001b[0m\u001b[0;32m     53\u001b[0m             \u001b[1;34m\"Unable to find a usable engine; \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;34m\"tried using: 'pyarrow', 'fastparquet'.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "# use os and os walk to navigate, not finished\n",
    "# use keys to make path, then use os to save path name\n",
    "# forget os.walk\n",
    "!pip install parquet\n",
    "for stock in statements:\n",
    "    try:\n",
    "        os.mkdir(stock)\n",
    "    except:\n",
    "        pass\n",
    "    for statement_type in statements[stock]:\n",
    " #       try:\n",
    "  #          os.mkdir(stock + '/' + statement_type)\n",
    "   #     except:\n",
    "    #        pass\n",
    "        pd.DataFrame(statements[stock][statement_type]).to_parquet(stock + \"/\" + statement_type + '/' + \".parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353ec7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9561d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stock in statements:\n",
    "    for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd5b9cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# below we have a summary of what our valuation tab looks like. For each\n",
    "# year it states the ROIC that we calculated, Earnings Per Share as defined\n",
    "# above, and an EPS_Valuation based on ROIC and EPS. This is essentially\n",
    "# a multiples analysis that assigns an earned P/E ratio based on Return On\n",
    "# Invested Capital. This is to reward companies that are generating more\n",
    "# value for less investment. \n",
    "\n",
    "statements[\"MBUU\"][\"VALUATION\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9854ac7d",
   "metadata": {},
   "source": [
    "# Pricing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff5d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yf.pdr_override()\n",
    "start = datetime.datetime(2012, 1, 1)\n",
    "end = datetime.datetime.today()\n",
    "\n",
    "data_dict = {}\n",
    "for key in stocks:\n",
    "    data_dict[key] = web.data.get_data_yahoo(key, start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9e51be",
   "metadata": {},
   "outputs": [],
   "source": [
    "close_data = pd.concat([val[\"Close\"]for val in data_dict.values()],\n",
    "                      keys = data_dict.keys(),\n",
    "                      axis = 1)\n",
    "close_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8892e866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this produces a traditional stock close chart\n",
    "plt.rcParams.update({'font.size': 32})\n",
    "fig, ax = plt.subplots(figsize = (24,16))\n",
    "close_data.plot.line(ax = ax, legend = True)\n",
    "y_vals = ax.get_yticks()\n",
    "ax.set_yticklabels([int(y) if y >= 1 else round(y,1) for y in y_vals])    \n",
    "\n",
    "plt.title(\"Daily Stock Prices\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562a71ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f776e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price chart data for each of the individual stocks. Hopefully can make double\n",
    "# graphs based on fundamental progress of the business like EPS or \n",
    "# revenue growth/EPS growth/margins etc.\n",
    "\n",
    "for stock in stocks:\n",
    "    plt.rcParams.update({'font.size': 32})\n",
    "    fig, ax = plt.subplots(figsize = (24,16))\n",
    "    close_data[stock].plot.line(ax = ax, legend = True)\n",
    "    y_vals = ax.get_yticks()\n",
    "    ax.set_yticklabels([int(y) if y >= 1 else round(y,1) for y in y_vals])    \n",
    "\n",
    "    plt.title(stock + \"\\nDaily Close Prices\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4718b6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this produces a log of stock prices chart\n",
    "plt.rcParams.update({'font.size': 32})\n",
    "fig, ax = plt.subplots(figsize = (24,16))\n",
    "close_data.plot.line(ax = ax, legend = True)\n",
    "ax.set_yscale(\"log\")\n",
    "y_vals = ax.get_yticks()\n",
    "ax.set_yticklabels([int(y) if y >= 1 else round(y,1) for y in y_vals])    \n",
    "\n",
    "plt.title(\"Daily Stock Prices\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94909e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to build a way to pull stock data for one stock at a time\n",
    "# still working on making this to pull multiple stocks at once\n",
    "# used for pricing data primarily\n",
    "\n",
    "def stock_data(stock):\n",
    "        data_dict = {}\n",
    "        stock = [stock]\n",
    "        for key in stock:\n",
    "            data_dict[key] = web.data.get_data_yahoo(key, start, end)\n",
    "        return data_dict\n",
    "    \n",
    "stock_data(\"MSFT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4378687",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enph = yf.Ticker(\"msft\")\n",
    "for key in dir(enph):\n",
    "    try:\n",
    "        print(key)\n",
    "        print()\n",
    "        print(getattr(msft, key))\n",
    "    except:\n",
    "        print(key, \"not accesible\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2786ef6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enph._earnings_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15e1202",
   "metadata": {},
   "source": [
    "# Other federal reserve data to be used in economic backdrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16885b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(data_codes,\n",
    "               start,\n",
    "               end = datetime.datetime.today(),\n",
    "               freq = \"M\"):\n",
    "    df_init = False\n",
    "    for key, code in data_codes.items():\n",
    "        if df_init == False:\n",
    "            # .first() or .last() .median() ...\n",
    "            df = web.DataReader(code, \"fred\", start, end).resample(freq).mean()            \n",
    "            df.rename(columns = {code:key}, inplace = True)\n",
    "            df_init = True\n",
    "        else:\n",
    "            df[key] = web.DataReader(code, \"fred\", start, end).resample(freq).mean()\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f461304",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime(1998,1,1)\n",
    "end = datetime.datetime.today()\n",
    "data_codes = {\"CPI\":\"FPCPITOTLZGUSA\"}\n",
    "gather_data(data_codes, start = start, end = end, freq = \"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb67cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
